groups:
  - name: disk_alerts
    rules:
      - alert: HighDiskUsage
        expr: |
          100 * (1 - (node_filesystem_free_bytes{device=~"/dev/.*", mountpoint="/"} / node_filesystem_size_bytes{device=~"/dev/.*", mountpoint="/"})) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Disk Usage (instance {{ $labels.instance }})"
          description: "Disk usage on instance {{ $labels.instance }} is above 85%."


groups:
  - name: kubernetes
    rules:
      - alert: APIServerHighRequestDuration
        expr: increase(apiserver_request_duration_seconds_sum[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High API Server Request Duration"
          description: "The API server request duration has been high for the last 5 minutes."



In this alerting rule, when the API server request duration sum increases by more than 10 seconds over the last 5 minutes continuously, a warning alert will be triggered. You can adjust the threshold (the > 10 part) and the duration (for: 5m) according to your specific performance requirements and tolerance for high request durations.

I questioned Arden of the Unix team about if he could see any problems with the iptables, specifically for ports 443 and 80, but he said he couldn't. He advised running the netstat command to see if the hostname brn was listening on port 80 and port 443, but the output showed that it was not. It looks that the server has not installed the httpd service.

---


groups:
  - name: kubernetes
    rules:
      - alert: APIServerHighRequestDuration
        expr: rate(apiserver_request_duration_seconds_sum[5m]) / rate(apiserver_request_duration_seconds_count[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Average API Server Request Duration"
          description: "The average API server request duration has been high for the last 5 minutes."


Capacity Planning:

Use insights from the average request duration to inform your capacity planning efforts. If you observe a consistent increase in request duration as the cluster approaches capacity, it may be time to consider scaling your cluster horizontally or vertically to handle the increased load.


---

rate(apiserver_request_duration_seconds_sum[5m]): This part calculates the per-second rate of change of the apiserver_request_duration_seconds_sum metric over a 5-minute time window. In other words, it determines how much the sum of request durations is increasing on average per second over the last 5 minutes.

rate(apiserver_request_duration_seconds_count[5m]): Similarly, this part calculates the per-second rate of change of the apiserver_request_duration_seconds_count metric over a 5-minute time window. It calculates how many requests are being processed on average per second over the last 5 minutes.

/: This is a division operator that divides the rate of change of apiserver_request_duration_seconds_sum by the rate of change of apiserver_request_duration_seconds_count. This division calculates the average request duration in seconds over the specified time window.

> 0.5: This is a comparison operator that checks whether the calculated average request duration is greater than 0.5 seconds.

So, when you use the expression rate(apiserver_request_duration_seconds_sum[5m]) / rate(apiserver_request_duration_seconds_count[5m]) > 0.5, you are essentially checking if, on average, requests to the Kubernetes API server are taking more than 0.5 seconds to complete over the last 5 minutes.

If the result of this expression is true, it means that the average request duration over the last 5 minutes exceeds the threshold of 0.5 seconds, indicating that requests are taking longer to process. This can be a useful condition to trigger an alert, as it suggests that the Kubernetes cluster may be under load or experiencing performance issues.

---

groups:
  - name: kubernetes
    rules:
      - alert: DeploymentReplicasUnavailable
        expr: kube_deployment_status_replicas_unavailable > 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Deployment Replicas Unavailable"
          description: "More than 1 replica is unavailable for the deployment."

---

The expression you provided, kube_deployment_status_replicas_unavailable{deployment="my-app-deployment|my-app-deployment2|deployment3"} > 1, is not valid for matching multiple deployments using a list of deployment names separated by | (pipe) within the curly braces. Prometheus does not support such a syntax for label matching.

To monitor multiple deployments with specific labels, you can use a regular expression, as previously mentioned. Here's an example of how you can modify the expression to monitor deployments labeled "my-app-deployment," "my-app-deployment2," and "deployment3" using a regular expression:

expr: kube_deployment_status_replicas_unavailable{deployment=~"my-app-deployment|my-app-deployment2|deployment3"} > 1

---

If you want to monitor multiple deployments with similar labels, you can use a regular expression in your expression to match those deployments.
For example, if you have deployments labeled with app=my-app and app=my-app2, you can use a regular expression like this:

yaml

expr: kube_deployment_status_replicas_unavailable{app=~"my-app|my-app2"} > 1

This expression will match deployments with either app=my-app or app=my-app2.

---

groups:
  - name: kubernetes
    rules:
      - alert: DeploymentReplicaMismatch
        expr: kube_deployment_status_replicas > kube_deployment_spec_replicas
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Deployment Replica Mismatch"
          description: "The current replica count exceeds the desired replica count."
