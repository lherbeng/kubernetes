High Load Average Metrics:

1. Analyze historical data trends of the node_load1 metric over a significant period to identify patterns and seasonal fluctuations.
2. Continuously monitor the performance metrics, including CPU usage, memory utilization, and disk I/O, in addition to the load average.
3. Identify the target system that requires the CPU performance upgrade.
4. Send a request to the appropriate team for a CPU upgrade
5. Please provide justification for your request why do you need to upgrade the CPU.
6. Find a suitable time for the CPU upgrade that won't disturb the team members using the server.
7. Run the command 'cat /proc/cpuinfo' to check the number of available CPU cores and ensure that the requested upgrade has been implemented.
8. Conduct thorough testing to validate the successful implementation and to confirm that the expected performance gains have been achieved.

---

Filesystem usage metrics:

1. Collect information about how we've used the disk over a long time, like many months or a whole year, so we can see how it's changed over different seasons and times.
2. Calculate how quickly the data and storage requirements are increasing by looking at the historical data over time.
3. Based on how fast things are growing and what we expect in the future, estimate how much storage we'll need in the coming months or years.
4. Regularly check the files on the server and remove any that are no longer needed to free up space.
5. Plan to add more space or resources, like extra storage, because we expect things to grow, and we have a limit we don't want to cross.
6. Kindly request a disk upgrade from the appropriate team.
7.  Please provide justification for your request why do you need to upgrade the disk.
8. Identify a convenient time for the disk upgrade that won't disrupt the server users.
9. The command "df -Th" can be executed to verify the available disk space and confirm the completion of the requested upgrade.

rate(node_filesystem_size_bytes{device="/dev/mapper/vg_data-rke2"}[1d])

---

Node memory high utilization metrics:

1. Check how the server is running to find which application or program are using a lot of memory. This can help you see where things can be made better.
2. Make sure your application or programare using your computer's resources efficiently. This might mean changing settings, how much memory different programs use, or using more efficient ways to handle data.
3. Use system monitoring tools like top to observe the memory usage of a specific process over an extended period
4. Keep an eye on how much memory your programs are using. Regularly checking can help you catch any slow increases in memory use that could cause problems over time.
5. Consider adding more memory to your server to manage the increased workload effectively. This upgrade can provide the necessary resources for improved performance and prevent slowdowns.
6. Kindly submit a request for memory upgrade from the appropriate team.
7. Please provide justification for your request why do you need to upgrade the RAM or memory.
8. Identify a convenient time for the RAM upgrade that won't disrupt the server users.
9. You can use the command "free -h" to check how much memory is available and confirm the completion of the requested upgrade.

---

Deployment replicas unavailable metrics:

1. Set up alerts that immediately notify the relevant team when this metric crosses a predefined threshold. This proactive approach can help prevent potential issues before they become critical.
2. Regularly assess the capacity of the on-premises hardware infrastructure. Ensure that the servers have sufficient CPU, memory, and storage to accommodate the workload demands and potential growth.
3.  Analyze Kubernetes logs and events to identify any specific errors or warnings related to the unavailable replicas. This can provide insights into the root cause of the issue.
4. To check the logs and events related to the unavailable replicas run the command "kubectl get events' and "kubectl logs <pod_name> to view the logs from a specific pod, and "kubectl describe pod <pod_name> to get a detailed description of a pod, including its current state and related events
5. Monitor the status of the nodes in the cluster. Verify if any specific nodes are experiencing issues or if there are any resource constraints leading to the unavailability of replicas.
6. Verify the deployment configurations to ensure that they align with the desired state and that the appropriate number of replicas is specified.
7. Implement health checks within the applications to identify and handle any issues that may cause the pods to become unavailable. Configure readiness and liveness probes to monitor the health of the application.

---

CPU high utilization metrics:

1. Check regularly if the CPU, memory, and storage are used properly across different parts of your system. Make sure they're used in the best possible way.
2. Analyze the performance of critical applications running on the cluster. Identify any resource-intensive applications that might be contributing to the increased CPU usage and take necessary optimization measures.
3. Monitor the network traffic within the cluster to identify any potential bottlenecks or issues related to network connectivity. Ensure that the network infrastructure is capable of handling the required data transfer rates without degradation.
4. Analyze the current workload patterns and resource utilization trends to determine if the increased CPU usage is a temporary spike or a persistent issue. 
5. Plan for the necessary downtime and system maintenance required for upgrading the CPU cores. Ensure that you have a well-defined strategy in place to minimize the impact on ongoing operations and to facilitate a smooth transition to the upgraded hardware.
6. Send a request to the appropriate team for a CPU cores upgrade.
7. Please provide justification for your request why do you need to upgrade the CPU.
6. Monitor the performance of the cluster after the CPU core upgrade to ensure that it effectively addresses the issue of exceeding the CPU threshold. Conduct thorough performance testing to validate the improvements and verify that the upgrade has resulted in the expected performance enhancements.

---

Pod CrashLooping Metrics:

1. Ensure that you are regularly monitoring the resource allocation and utilization of your on-prem server. This involves keeping an eye on CPU, memory, and disk usage. 
2. Ensure that the resource requests and limits are appropriately configured for your Kubernetes pods. Setting appropriate resource requests and limits ensures that your pods have enough resources to run while preventing them from consuming excess resources, which can lead to performance issues or crashes.
3. Dive deeper into the logs of the application running inside the problematic pod. Identify the specific error that's causing the CrashLoopBackOff state. Often, detailed error logs can provide clues about the underlying issue, such as resource contention, misconfigurations, or other issues with dependencies.
4. CrashLoopBackOff indicates that a container is repeatedly crashing after restarting. A container might crash for many reasons, and checking a Pod's logs might aid in troubleshooting the root cause.
5. To see all Pods running in your cluster, run "kubectl get pods" and look for the Pod with the CrashLoopBackOff error and execute the "kubectl logs <pod_name> to get the Pod's logs
6. Configure readiness and liveness probes to monitor the health of the application.

---
 
Kube Node Not Ready Metrics:

1. A node must have enough disk space, memory, and processing power to run Kubernetes workloads. If non-Kubernetes processes on the node are taking up too many resources, or if there are too many processes running on the node, it can be marked by the control plane as NotReady. Run "kubectl describe node" and look in the Conditions section to see if resources are missing on the node:

MemoryPressure — node is running out of memory.
DiskPressure — node is running out of disk space.
PIDPressure — node is running too many processes.

2. You can create alerting rules in Grafana to monitor the MemoryPressure, DiskPressure, and PIDPressure for nodes.
3. The kubelet must run on each node to enable it to participate in the cluster. If the kubelet crashes or stops on a node, it cannot communicate with the API server and the node goes into a not ready state. Run "kubectl describe node [name]" and look in the Conditions section and if all the conditions are unknown, this indicates the kubelet is down.

To resolve a kubelet issue, SSH into the node and run the command systemctl status kubelet

Look at the value of the Active field:

active (running) means the kubelet is actually operational, look for the problem elsewhere.
active (exited) means the kubelet was exited, probably in error. Restart it.
inactive (dead) means the kubelet crashed. To identify why, run the command journalctl -u kubelet and examine the kubelet logs.

4. kube-proxy runs on every node and is responsible for regulating network traffic between the node and other entities inside and outside the cluster. If kube-proxy stops running for any reason, the node goes into a not ready state. Run kubectl get pods -n kube-system to show pods belonging to the Kubernetes system.

Try looking in the following places to identify what is the issue with kube-proxy:

Run the command kubectl describe pod using the name of the kube-proxy pod that failed, and check the Events section in the output.
Run the command kubectl logs [pod-name] -n kube-system to see a full log of the failing kube-proxy pod.
Run the command kubectl describe daemonset kube-proxy -n kube-system to see the status of the kube-proxy daemonset, which is responsible for ensuring there is a kube-proxy running on every Kubernetes node.

5. Even if a node is configured perfectly, but it has no network connectivity, Kubernetes treats the node as not ready. This could be due to a disconnected network cable, no Internet access, or misconfigured networking on the machine. Run kubectl describe node [name] and look in the Conditions section—if the NetworkUnavailable flag is True, this means the node has a connectivity issue.
6. Identify which non-Kubernetes processes are running on the node. If there are any, shut them down or reduce them to a minimum to conserve resources.



